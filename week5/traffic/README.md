One convolutional layer with 32 filters and 3 x 3 kernel and one max-pooling layer with a 2 x 2 size were explored with different numbers of hidden layers and different values of the dropout rate.

First a hidden layer with 32 units and a dropout rate of 0.5 was explored. The result was approximately 6% accuracy with a loss value of 3.49. From here I increased the values for the hidden layer units and then explored decreasing the dropout rate. When the hidden layer was changed to 64 units, the accuracy increased greatly to 50% and a loss value dropped to 1.5. Next, when the hidden layer was changed to 238 units, the accuracy increased slightly, to 69% and loss dropped to 0.93. For the rest of the exploration, 128 units was used for the hidden layers.

The next step was exploring an additional hidden layer with 128 units. With two layers, the accuracy was 0.74%, loss value to 0.82. With three layers, the accuracy was 89% and the loss value 0.37.With four layers, the accuracy was 94% and the loss value 0.23. Surprisingly, with five hidden layers, the accuracy was dropped to 92% and the loss value increased to 0.29. For the rest of the exploration, 4 layers were used.

Then, different dropout rates were explored with four hidden layers, each at 128 units. At the dropout rate of 1, an error was produced, so 0.9 was tried instead which produced 33% accuracy and loss value of 2.26. At the rate of 0.7, the accuracy was 70% and the loss value at 0.91. The dropout value of 0.5 was already explored as explained above. At the dropout rate of 0.3, the accuracy increased to 93% and the loss value decreased to 0.24. At the dropout rate of 0.1, the accuracy increased slightly to 0.96 and the loss value decreased to 0.17. 
